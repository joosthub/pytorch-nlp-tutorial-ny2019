{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4' \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header\n",
    "\n",
    "In this notebook, we outline how we approach loading and vectorizing datasets.  \n",
    "\n",
    "Loading and vectorizing a dataset consists of 3 components:\n",
    "\n",
    "1. A data structure named `Vocabulary` which manages the token to integer mapping\n",
    "2. A `Vectorizer` which manages the vocabulary (or vocabularies) for mapping data points to a vector of integers\n",
    "3. The Dataset itself which takes as input a dataframe and a vectorizer.  For classification tasks, it's expected that there are 2 columns, one for the observation, and one for the label.\n",
    "\n",
    "To prepare for those 3 components, we do the following tasks:\n",
    "\n",
    "1. Annotate the dataset with split information. \n",
    "2. Preprocess and split the x data (the observations) into lists of tokens\n",
    "3. Count the tokens and use the counts to restrict the vectorized tokens to those that are frequent enough to learn from\n",
    "\n",
    "From there, creating the vectorizer is as simple as iterating through the counted tokens.  Then, the vectorizer is used to transform each subset of the dataset (corresponding to the splits) into matrices of token integers and vectors of label indices. \n",
    "\n",
    "### Dataset Information\n",
    "\n",
    "- Name: Surnames \n",
    "- Fields: `surname`, `nationality`\n",
    "- Size: 10980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args\n",
    "\n",
    "We utilize the `Namespace` object from python's standard library to contain hyper parameters and runtime settings.  Primarily, this is done because it plays well with static analyzers and can be serialized for distributed settings. It's also convenient because it gives attribute-access rather than key-based access of dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    surname_csv=\"../data/surnames.csv\"\n",
    ")\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(x_data_list):\n",
    "    \"\"\"Count the tokens in the data list\n",
    "    \n",
    "    Args:\n",
    "        x_data_list (list(list(str))): a list of lists, each sublist is a list of string tokens. \n",
    "            In other words, a list of the data points where the data points have been tokenized.\n",
    "    Returns:\n",
    "        dict: a mapping from tokens to their counts \n",
    "    \n",
    "    \"\"\"\n",
    "    # alternatively\n",
    "    # return Counter([token for x_data in x_data_list for token in x_data])\n",
    "    counter = Counter()\n",
    "    for x_data in x_data_list:\n",
    "        for token in x_data:\n",
    "            counter[token] += 1\n",
    "    return counter\n",
    "\n",
    "def add_splits(df, target_y_column, split_proportions=(0.7, 0.15, 0.15), seed=0):\n",
    "    \"\"\"Add 'train', 'val', and 'test' splits to the dataset\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the data frame to assign splits to\n",
    "        target_y_column (str): the name of the label column; in order to\n",
    "            preserve the class distribution between splits, the label column\n",
    "            is used to group the datapoints and splits are assigned within these groups.\n",
    "        split_proportions (tuple(float, float, float)): three floats which represent the\n",
    "            proportion in 'train', 'val, 'and 'test'. Must sum to 1. \n",
    "        seed (int): the random seed for making the shuffling deterministic. If the dataset and seed\n",
    "            are kept the same, the split assignment is deterministic. \n",
    "    Returns:\n",
    "        pd.DataFrame: the input dataframe with a new column for split assignments; note: row order\n",
    "            will have changed.\n",
    "            \n",
    "    \"\"\"\n",
    "    df_by_label = {label: [] for label in df[target_y_column].unique()}\n",
    "    for _, row in df.iterrows():\n",
    "        df_by_label[row[target_y_column]].append(row.to_dict())\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    assert sum(split_proportions) == 1, \"`split_proportions` should sum to 1\"\n",
    "    train_p, val_p, test_p = split_proportions\n",
    "    \n",
    "    out_df = []\n",
    "    # to ensure consistent behavior, lexicographically sort the dictionary\n",
    "    for _, data_points in sorted(df_by_label.items()):\n",
    "        np.random.shuffle(data_points)\n",
    "        n_total = len(data_points)\n",
    "        n_train = int(train_p * n_total)\n",
    "        n_val = int(val_p * n_total)\n",
    "        \n",
    "        for data_point in data_points[:n_train]:\n",
    "            data_point['split'] = 'train'\n",
    "            \n",
    "        for data_point in data_points[n_train:n_train+n_val]:\n",
    "            data_point['split'] = 'val'\n",
    "            \n",
    "        for data_point in data_points[n_train+n_val:]:\n",
    "            data_point['split'] = 'test'\n",
    "        \n",
    "        out_df.extend(data_points)\n",
    "    \n",
    "    return pd.DataFrame(out_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedTextVectorizer:\n",
    "    \"\"\"A composite data structure that uses Vocabularies to map text and its labels to integers\n",
    "    \n",
    "    Attributes:\n",
    "        token_vocab (Vocabulary): the vocabulary managing the mapping between text tokens and \n",
    "            the unique indices that represent them\n",
    "        label_voab (Vocabulary): the vocabulary managing the mapping between labels and the\n",
    "            unique indices that represent them.\n",
    "        max_seq_length (int): the length of the longest sequence (including start or end tokens\n",
    "            that will be prepended or appended).\n",
    "    \"\"\"\n",
    "    def __init__(self, token_vocab, label_vocab, max_seq_length):\n",
    "        \"\"\"Initialize the SupervisedTextVectorizer\n",
    "        \n",
    "        Args:\n",
    "            token_vocab (Vocabulary): the vocabulary managing the mapping between text tokens and \n",
    "                the unique indices that represent them\n",
    "            label_voab (Vocabulary): the vocabulary managing the mapping between labels and the\n",
    "                unique indices that represent them.\n",
    "            max_seq_length (int): the length of the longest sequence (including start or end tokens\n",
    "                that will be prepended or appended).\n",
    "        \"\"\"\n",
    "        self.token_vocab = token_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def _wrap_with_start_end(self, x_data):\n",
    "        \"\"\"Prepend the start token and append the end token.\n",
    "        \n",
    "        Args:\n",
    "            x_data (list(str)): the list of string tokens in the data point\n",
    "        Returns:\n",
    "            list(str): the list of string tokens with start token prepended and end token appended\n",
    "        \"\"\"\n",
    "        return [self.token_vocab.start_token] + x_data + [self.token_vocab.end_token]\n",
    "    \n",
    "    def vectorize(self, x_data, y_label):\n",
    "        \"\"\"Convert the data point and its label into their integer form\n",
    "        \n",
    "        Args:\n",
    "            x_data (list(str)): the list of string tokens in the data point\n",
    "            y_label (str,int): the label associated with the data point\n",
    "        Returns:\n",
    "            numpy.ndarray, int: x_data in vector form, padded to the max_seq_length; and \n",
    "                the label mapped to the integer that represents it\n",
    "        \"\"\"\n",
    "        x_data = self._wrap_with_start_end(x_data)\n",
    "        x_vector = np.zeros(self.max_seq_length).astype(np.int64)\n",
    "        x_data_indices = [self.token_vocab[token] for token in x_data]\n",
    "        x_vector[:len(x_data_indices)] = x_data_indices\n",
    "        y_index = self.label_vocab[y_label]\n",
    "        return x_vector, y_index\n",
    "    \n",
    "    def transform(self, x_data_list, y_label_list):\n",
    "        \"\"\"Transform a dataset by vectorizing each datapoint\n",
    "        \n",
    "        Args: \n",
    "            x_data_list (list(list(str))): a list of lists, each sublist contains string tokens\n",
    "            y_label_list (list(str,int)): a list of either strings or integers. the y label can come\n",
    "                as strings or integers, but they are remapped with the label_vocab to a unique integer\n",
    "        Returns:\n",
    "            np.ndarray(matrix), np.ndarray(vector): the vectorized x (matrix) and vectorized y (vector) \n",
    "        \"\"\"\n",
    "        x_matrix = []\n",
    "        y_vector = []\n",
    "        for x_data, y_label in zip(x_data_list, y_label_list):\n",
    "            x_vector, y_index = self.vectorize(x_data, y_label)\n",
    "            x_matrix.append(x_vector)\n",
    "            y_vector.append(y_index)\n",
    "        \n",
    "        return np.stack(x_matrix), np.stack(y_vector)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls, df, target_x_column, target_y_column, token_count_cutoff=0):\n",
    "        \"\"\"Instantiate the SupervisedTextVectorizer from a standardized dataframe\n",
    "        \n",
    "        Standardized DataFrame has a special meaning:\n",
    "            there is a column that has been tokenized into a list of strings\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataset with a tokenized text column and a label column\n",
    "            target_x_column (str): the name of the tokenized text column\n",
    "            target_y_column (str): the name of the label column\n",
    "            token_count_cutoff (int): [default=0] the minimum token frequency to add to the\n",
    "                token_vocab.  Any tokens that are less frequent will not be added.\n",
    "        Returns:\n",
    "            SupervisedTextVectorizer: the instantiated vectorizer\n",
    "        \"\"\"\n",
    "        # get the x data (the observations)\n",
    "        target_x_list = df[target_x_column].tolist()\n",
    "        # compute max sequence length, add 2 for the start, end tokens\n",
    "        max_seq_length = max(map(len, target_x_list)) + 2 \n",
    "        \n",
    "        # populate token vocab        \n",
    "        token_vocab = Vocabulary(use_unks=False,\n",
    "                                 use_mask=True,\n",
    "                                 use_start_end=True,\n",
    "                                 start_token=START_TOKEN,\n",
    "                                 end_token=END_TOKEN)\n",
    "        counts = count_tokens(target_x_list)\n",
    "        # sort counts in reverse order\n",
    "        for token, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count < token_count_cutoff:\n",
    "                break\n",
    "            token_vocab.add(token)\n",
    "\n",
    "        # populate label vocab\n",
    "        label_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "        # add the sorted unique labels \n",
    "        label_vocab.add_many(sorted(df[target_y_column].unique()))\n",
    "        \n",
    "        return cls(token_vocab, label_vocab, max_seq_length)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the vectorizer using json to the file specified\n",
    "        \n",
    "        Args:\n",
    "            filename (str): the output file\n",
    "        \"\"\"\n",
    "        vec_dict = {\"token_vocab\": self.token_vocab.get_serializable_contents(),\n",
    "                    \"label_vocab\": self.label_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"wb\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        \"\"\"Load the vectorizer from the json file it was saved to\n",
    "        \n",
    "        Args:\n",
    "            filename (str): the file into which the vectorizer was saved.\n",
    "        Returns:\n",
    "            SupervisedTextVectorizer: the instantiated vectorizer\n",
    "        \"\"\"\n",
    "        with open(filename, \"rb\") as fp:\n",
    "            contents = json.load(fp)\n",
    "\n",
    "        contents[\"token_vocab\"] = Vocabulary.deserialize_from_contents(contents[\"token_vocab\"])\n",
    "        contents[\"label_vocab\"] = Vocabulary.deserialize_from_contents(contents[\"label_vocab\"])\n",
    "        return cls(**contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        vectorizer (SupervisedTextVectorizer): an instantiated vectorizer\n",
    "        active_split (str): the string name of the active split\n",
    "        \n",
    "        # internal use\n",
    "        _split_df (dict): a mapping from split name to partitioned DataFrame\n",
    "        _vectorized (dict): a mapping from split to an x data matrix and y vector\n",
    "        _active_df (pd.DataFrame): the DataFrame corresponding to the split\n",
    "        _active_x (np.ndarray): a matrix of the vectorized text data\n",
    "        _active_y (np.ndarray): a vector of the vectorized labels\n",
    "    \"\"\"\n",
    "    def __init__(self, df, vectorizer, target_x_column, target_y_column):\n",
    "        \"\"\"Initialize the SupervisedTextDataset\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataset with a text and label column\n",
    "            vectorizer (SupervisedTextVectorizer): an instantiated vectorizer\n",
    "            target_x_column (str): the column containing the tokenized text\n",
    "            target_y_column (str): the column containing the label\n",
    "        \"\"\"\n",
    "        self._split_df = {\n",
    "            'train': df[df.split=='train'],\n",
    "            'val': df[df.split=='val'],\n",
    "            'test': df[df.split=='test']\n",
    "        }\n",
    "        \n",
    "        self._vectorized = {}\n",
    "        for split_name, split_df in self._split_df.items():\n",
    "            self._vectorized[split_name] = \\\n",
    "                vectorizer.transform(x_data_list=split_df[target_x_column].tolist(), \n",
    "                                     y_label_list=split_df[target_y_column].tolist())\n",
    "        self.vectorizer = vectorizer\n",
    "        self.active_split = None\n",
    "        self._active_df = None\n",
    "        self._active_x = None\n",
    "        self._active_y = None\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    def set_split(self, split_name):\n",
    "        \"\"\"Set the active split\n",
    "        \n",
    "        Args:\n",
    "            split_name (str): the name of the split to make active; should\n",
    "                be one of 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self.active_split = split_name\n",
    "        self._active_x, self._active_y = self._vectorized[split_name]\n",
    "        self._active_df = self._split_df[split_name]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return the data point corresponding to the index\n",
    "        \n",
    "        Args:\n",
    "            index (int): an int between 0 and len(self._active_x)\n",
    "        Returns:\n",
    "            dict: the data for this data point. Has the following form:\n",
    "                {\"x_data\": the vectorized text data point, \n",
    "                 \"y_target\": the index of the label for this data point, \n",
    "                 \"x_lengths\": method: the number of nonzeros in the vector,\n",
    "                 \"data_index\": the provided index for bookkeeping}\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"x_data\": self._active_x[index],\n",
    "            \"y_target\": self._active_y[index],\n",
    "            \"x_lengths\": len(self._active_x[index].nonzero()[0]),\n",
    "            \"data_index\": index\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"The length of the active dataset\n",
    "        \n",
    "        Returns:\n",
    "            int: len(self._active_x)\n",
    "        \"\"\"\n",
    "        return self._active_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_tokenizer(input_string):\n",
    "    \"\"\"Tokenized a string a list of its characters\n",
    "    \n",
    "    Args:\n",
    "        input_string (str): the character string to tokenize\n",
    "    Returns:\n",
    "        list: a list of characters\n",
    "    \"\"\"\n",
    "    return list(input_string.lower())\n",
    "\n",
    "def load_surname_dataset(dataset_csv, tokenizer_func, saved_vectorizer_file=None):\n",
    "    \"\"\"Load the surname dataset \n",
    "    \n",
    "    Args:\n",
    "        dataset_csv (str): the location of the dataset\n",
    "        tokenizer_func (function): the tokenizing function to turn each datapoint into \n",
    "            its tokenized form\n",
    "        saved_vectorizer_file (str or None): [default=None] if not None, load the vectorizer\n",
    "            from the file\n",
    "    \"\"\"\n",
    "    df = add_splits(pd.read_csv(dataset_csv), 'nationality')\n",
    "    df['tokenized'] = df.surname.apply(tokenizer_func)\n",
    "    if saved_vectorizer_file is not None:\n",
    "        vectorizer = SupervisedTextVectorizer.load(saved_vectorizer_file)\n",
    "    else:\n",
    "        vectorizer = SupervisedTextVectorizer.from_df(df, \n",
    "                                                      target_x_column='tokenized', \n",
    "                                                      target_y_column='nationality')\n",
    "    dataset = SupervisedTextDataset(df=df, \n",
    "                                    vectorizer=vectorizer, \n",
    "                                    target_x_column='tokenized', \n",
    "                                    target_y_column='nationality')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_surname_dataset(args.surname_csv, tokenizer_func=character_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating batches\n",
    "\n",
    "Finally, the make_data_generator interacts with PyTorch's `DataLoader` and returns a generator. It basically just iterates over the `DataLoader` generator and does some processing.  Currently, it returns a function rather than just making the generator itself so some control can be had over num_batches & volatile mode, and other run time things. It's mostly a cheap and easy function that can be written in many ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\", dataloader_kwargs=None): \n",
    "    \"\"\"Generate batches from a dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): the instantiated dataset\n",
    "        batch_size (int): the size of the batches\n",
    "        shuffle (bool): [default=True] batches are formed from shuffled indices\n",
    "        drop_last (bool): [default=True] don't return the final batch if it's smaller\n",
    "            than the specified batch size\n",
    "        device (str): [default=\"cpu\"] the device to move the tensors to\n",
    "        dataloader_kwargs (dict or None): [default=None] Any additional arguments to the\n",
    "            DataLoader can be specified\n",
    "    Yields:\n",
    "        dict: a dictionary mapping from tensor name to tensor object where the first\n",
    "            dimension of tensor object is the batch dimension\n",
    "    Note: \n",
    "        This function is mostly an iterator for the DataLoader, but has the added\n",
    "        feature that it moves the tensors to a target device. \n",
    "    \"\"\"\n",
    "    dataloader_kwargs = dataloader_kwargs or {}\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last, **dataloader_kwargs)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"A data structure for managing training state operations.\n",
    "    \n",
    "    The TrainState will monitor validation loss and everytime a new best loss\n",
    "        (lower is better) is observed, a couple things happen:\n",
    "        \n",
    "        1. The model is checkpointed\n",
    "        2. Patience is reset\n",
    "    \n",
    "    Attributes:\n",
    "        model (torch.nn.Module): the model being trained and will be\n",
    "            checkpointed during training.\n",
    "        dataset (SupervisedTextDataset, TextSequenceDataset): the dataset \n",
    "            which is being iterate during training; must have the `active_split`\n",
    "            attribute. \n",
    "        log_dir (str): the directory to output the checkpointed model \n",
    "        patience (int): the number of epochs since a new best loss was observed\n",
    "        \n",
    "        # Internal Use\n",
    "        _full_model_path (str): `log_dir/model_state_file`\n",
    "        _split (str): the active split\n",
    "        _best_loss (float): the best observed loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataset, log_dir, model_state_file=\"model.pth\"):\n",
    "        \"\"\"Initialize the TrainState\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): the model to be checkpointed during training\n",
    "            dataset (SupervisedTextDataset, TextSequenceDataset): the dataset \n",
    "                which is being iterate during training; must have the `active_split`\n",
    "                attribute. \n",
    "            log_dir (str): the directory to output the checkpointed model \n",
    "            model_state_file (str): the name of the checkpoint model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self._full_model_path = os.path.join(log_dir, model_state_file)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self._metrics_by_split = {\n",
    "            'train': {}, \n",
    "            'val': {}, \n",
    "            'test': {}\n",
    "        }\n",
    "        \n",
    "        self._split = 'train'\n",
    "        self._best_loss = 10**10\n",
    "        self.patience = 0\n",
    "        \n",
    "    def _init_metric(self, split, metric_name):\n",
    "        \"\"\"Initialize a metric to the specified split\n",
    "        \n",
    "        A dictionary is created in `self._metrics_by_split` with\n",
    "            the keys 'running', 'count', and 'history'. \n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split to record the metric\n",
    "            metric_name (str): the name of the metric\n",
    "        \"\"\"\n",
    "        self._metrics_by_split[split][metric_name] = {\n",
    "            'running': 0.,\n",
    "            'count': 0,\n",
    "            'history': []\n",
    "        }\n",
    "        \n",
    "    def _update_metric(self, metric_name, metric_value):\n",
    "        \"\"\"Update a metric with an observed value\n",
    "        \n",
    "        Specifically, the running average is updated.\n",
    "        \n",
    "        Args:\n",
    "            metric_name (str): the name of the metric\n",
    "            metric_value (float): the observed value of the metric\n",
    "        \"\"\"\n",
    "        if metric_name not in self._metrics_by_split[self._split]:\n",
    "            self._init_metric(self._split, metric_name)\n",
    "        metric = self._metrics_by_split[self._split][metric_name]\n",
    "        metric['count'] += 1\n",
    "        metric['running'] += (metric_value - metric['running']) / metric['count']\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        \"\"\"Set the dataset split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split to set\n",
    "        \"\"\"\n",
    "        self._split = split\n",
    "        \n",
    "    def get_history(self, split, metric_name):\n",
    "        \"\"\"Get the history of values for any metric in any split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split\n",
    "            metric_name (str): the target metric\n",
    "            \n",
    "        Returns:\n",
    "            list(float): the running average of each epoch for `metric_name` in `split` \n",
    "        \"\"\"\n",
    "        return self._metrics_by_split[split][metric_name]['history']\n",
    "    \n",
    "    def get_value_of(self, split, metric_name):\n",
    "        \"\"\"Retrieve the running average of any metric in any split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split\n",
    "            metric_name (str): the target metric\n",
    "            \n",
    "        Returns:\n",
    "            float: the running average for `metric_name` in `split`\n",
    "        \"\"\"\n",
    "        return self._metrics_by_split[split][metric_name]['running']\n",
    "        \n",
    "    def log_metrics(self, **metrics):\n",
    "        \"\"\"Log some values for some metrics\n",
    "        \n",
    "        Args:\n",
    "            metrics (kwargs): pass keyword args with the form `metric_name=metric_value`\n",
    "                to log the metric values into the attribute `_metrics_by_split`.\n",
    "        \"\"\"\n",
    "        self._split = self.dataset.active_split\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            self._update_metric(metric_name, metric_value)\n",
    "            \n",
    "    def log_epoch_end(self):\n",
    "        \"\"\"Log the end of the epoch. \n",
    "        \n",
    "        Some key functions happen at the end of the epoch:\n",
    "            - for each metric in each split running averages, counts, \n",
    "              and history are updated\n",
    "            - the model is checkpointed if a new best value is observed\n",
    "            - patience is incremented if a new best value is not observed\n",
    "        \"\"\"\n",
    "        for split_dict in self._metrics_by_split.values():\n",
    "            for metric_dict in split_dict.values():\n",
    "                metric_dict['history'].append(metric_dict['running'])\n",
    "                metric_dict['running'] = 0.0\n",
    "                metric_dict['count'] = 0\n",
    "                \n",
    "        if 'loss' in self._metrics_by_split['val']:\n",
    "            val_loss = self._metrics_by_split['val']['loss']['history'][-1]\n",
    "            if val_loss < self._best_loss:\n",
    "                self._best_loss = val_loss\n",
    "                self.save_model()\n",
    "                self.patience = 0\n",
    "            else:\n",
    "                self.patience += 1\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" Save `model` to `log_dir/model_state_file` \"\"\"\n",
    "        torch.save(self.model.state_dict(), self._full_model_path)\n",
    "    \n",
    "    def reload_best(self):\n",
    "        \"\"\" reload `log_dir/model_state_file` to `model` \"\"\"\n",
    "        if os.path.exists(self._full_model_path):\n",
    "            self.model.load_state_dict(torch.load(self._full_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data has shape torch.Size([4, 22])\n",
      "y_target has shape torch.Size([4])\n",
      "x_lengths has shape torch.Size([4])\n",
      "data_index has shape torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch_generator = generate_batches(dataset, batch_size=4)\n",
    "batch = next(batch_generator)\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key} has shape {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.1666666666666665\n",
      "1.1666666666666667\n",
      "[1.1666666666666667]\n",
      "[2.1666666666666665]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "model = torch.nn.Linear(10,10)\n",
    "train_state = TrainState(model, dataset, 'model.pth', './logs')\n",
    "dataset.set_split(\"train\")\n",
    "train_state.log_metrics(loss=1.0)\n",
    "train_state.log_metrics(loss=2.0)\n",
    "train_state.log_metrics(loss=0.5)\n",
    "\n",
    "dataset.set_split(\"val\")\n",
    "train_state.log_metrics(loss=1.5)\n",
    "train_state.log_metrics(loss=2.5)\n",
    "train_state.log_metrics(loss=2.5)\n",
    "\n",
    "print(train_state.get_value_of(\"val\", \"loss\"))\n",
    "print(train_state.get_value_of(\"train\", \"loss\"))\n",
    "train_state.log_epoch_end()\n",
    "\n",
    "print(train_state.get_history(split='train', metric_name='loss'))\n",
    "print(train_state.get_history(split='val', metric_name='loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magis",
   "language": "python",
   "name": "magis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
