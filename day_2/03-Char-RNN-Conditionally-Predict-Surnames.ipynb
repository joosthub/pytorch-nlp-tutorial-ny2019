{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Surname Generation with RNNs\n",
    "\n",
    "## Dataset Info\n",
    "\n",
    "The surnames dataset has been collected from a couple different sources. \n",
    "\n",
    "#### Value Counts for the Nationality:\n",
    "\n",
    "```\n",
    "russian       9408\n",
    "english       3668\n",
    "arabic        2000\n",
    "japanese       991\n",
    "german         724\n",
    "italian        709\n",
    "czech          519\n",
    "spanish        298\n",
    "dutch          297\n",
    "french         277\n",
    "chinese        268\n",
    "irish          232\n",
    "greek          203\n",
    "polish         139\n",
    "scottish       100\n",
    "korean          94\n",
    "portuguese      74\n",
    "vietnamese      73\n",
    "Name: nationality, dtype: int64\n",
    "```\n",
    "\n",
    "## Model Info\n",
    "\n",
    "The `ConditionalCharRNN` first conditions on an embedding of the nationality and is then trained to generate the surnames.  In this way, the model can learn nationality-specific representations for surnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4' \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\"\n",
    "IGNORE_INDEX_VALUE = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(x_data_list):\n",
    "    \"\"\"Count the tokens in the data list\n",
    "    \n",
    "    Args:\n",
    "        x_data_list (list(list(str))): a list of lists, each sublist is a list of string tokens. \n",
    "            In other words, a list of the data points where the data points have been tokenized.\n",
    "    Returns:\n",
    "        dict: a mapping from tokens to their counts \n",
    "    \n",
    "    \"\"\"\n",
    "    # alternatively\n",
    "    # return Counter([token for x_data in x_data_list for token in x_data])\n",
    "    counter = Counter()\n",
    "    for x_data in x_data_list:\n",
    "        for token in x_data:\n",
    "            counter[token] += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def add_splits(df, target_y_column=None, split_proportions=(0.7, 0.15, 0.15), seed=0):\n",
    "    \"\"\"Add 'train', 'val', and 'test' splits to the dataset\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the data frame to assign splits to\n",
    "        target_y_column (str): [default=None] \n",
    "            the name of the label column; in order to preserve the class distribution \n",
    "            between splits, the label column is used to group the datapoints and splits are \n",
    "            assigned within these groups. If None, the assumption is this is an unsupervised\n",
    "            task and the splits will not respect any class distribution\n",
    "        split_proportions (tuple(float, float, float)): three floats which represent the\n",
    "            proportion in 'train', 'val, 'and 'test'. Must sum to 1. \n",
    "        seed (int): the random seed for making the shuffling deterministic. If the dataset and seed\n",
    "            are kept the same, the split assignment is deterministic. \n",
    "    Returns:\n",
    "        pd.DataFrame: the input dataframe with a new column for split assignments; note: row order\n",
    "            will have changed\n",
    "    \"\"\"\n",
    "    if target_y_column is not None:\n",
    "        # partition on the y label\n",
    "        df_by_label = {label: [] for label in df[target_y_column].unique()}\n",
    "        for _, row in df.iterrows():\n",
    "            df_by_label[row[target_y_column]].append(row.to_dict())\n",
    "    else:\n",
    "        # no y label to partition on\n",
    "        df_by_label = dict(unlabeled=df.to_dict('records'))\n",
    "            \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    assert sum(split_proportions) == 1, \"`split_proportions` should sum to 1\"\n",
    "    train_p, val_p, test_p = split_proportions\n",
    "    \n",
    "    out_df = []\n",
    "    # to ensure consistent behavior, lexicographically sort the dictionary\n",
    "    for _, data_points in sorted(df_by_label.items()):\n",
    "        np.random.shuffle(data_points)\n",
    "        n_total = len(data_points)\n",
    "        n_train = int(train_p * n_total)\n",
    "        n_val = int(val_p * n_total)\n",
    "        \n",
    "        for data_point in data_points[:n_train]:\n",
    "            data_point['split'] = 'train'\n",
    "            \n",
    "        for data_point in data_points[n_train:n_train+n_val]:\n",
    "            data_point['split'] = 'val'\n",
    "            \n",
    "        for data_point in data_points[n_train+n_val:]:\n",
    "            data_point['split'] = 'test'\n",
    "        \n",
    "        out_df.extend(data_points)\n",
    "    \n",
    "    return pd.DataFrame(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Sequence Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSequenceVectorizer:\n",
    "    \"\"\"A composite data structure that uses Vocabularies to map text and its labels to integers\n",
    "    \n",
    "    This differs from the SupervisedTextVectorizer in that it outputs the sequences\n",
    "    in a source-target pairing: the source is all indices except the final index and\n",
    "    the target is all indices except the first index.  In this pairing, the computational\n",
    "    goal is to learn a function which maps source to target at every timestep. \n",
    "    \n",
    "    The variable name of the label has changed in various places as well. This is to indicate\n",
    "    that the label column is no longer the y-variable of the machine learning problem. \n",
    "    \n",
    "    Attributes:\n",
    "        token_vocab (Vocabulary): the vocabulary managing the mapping between text tokens and \n",
    "            the unique indices that represent them\n",
    "        label_voab (Vocabulary): the vocabulary managing the mapping between labels and the\n",
    "            unique indices that represent them.\n",
    "        max_seq_length (int): the length of the longest sequence (including start or end tokens\n",
    "            that will be prepended or appended).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token_vocab, label_vocab, max_seq_length):\n",
    "        \"\"\"Initialize the TextSequenceVectorizer\n",
    "        \n",
    "        Args:\n",
    "            token_vocab (Vocabulary): the vocabulary managing the mapping between text tokens and \n",
    "                the unique indices that represent them\n",
    "            label_voab (Vocabulary): the vocabulary managing the mapping between labels and the\n",
    "                unique indices that represent them.\n",
    "            max_seq_length (int): the length of the longest sequence (including start or end tokens\n",
    "                that will be prepended or appended).\n",
    "        \"\"\"\n",
    "        self.token_vocab = token_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def _wrap_with_start_end(self, x_data):\n",
    "        \"\"\"Prepend the start token and append the end token.\n",
    "        \n",
    "        Args:\n",
    "            x_data (list(str)): the list of string tokens in the data point\n",
    "        Returns:\n",
    "            list(str): the list of string tokens with start token prepended and end token appended\n",
    "        \"\"\"\n",
    "        return [self.token_vocab.start_token] + x_data + [self.token_vocab.end_token]\n",
    "    \n",
    "    def vectorize(self, x_data, label):\n",
    "        \"\"\"Convert the data point and its label into their integer form\n",
    "        \n",
    "        Args:\n",
    "            x_data (list(str)): the list of string tokens in the data point\n",
    "            label (str,int): the label associated with the data point\n",
    "        Returns:\n",
    "            numpy.ndarray, np.ndarray, int: \n",
    "                the first two outputs are x_data in two vectorized forms:\n",
    "                    the first is the source vector (x_data[:-1])\n",
    "                    the second is the target vector (x_data[1:])\n",
    "                the third output is the label mapped to the integer that represents it\n",
    "        \"\"\"\n",
    "        x_data = self._wrap_with_start_end(x_data)\n",
    "        \n",
    "        x_source_vector = np.zeros(self.max_seq_length).astype(np.int64)\n",
    "        y_target_vector = np.ones(self.max_seq_length).astype(np.int64) * IGNORE_INDEX_VALUE\n",
    "        \n",
    "        x_data_indices = [self.token_vocab[token] for token in x_data]\n",
    "        x_source_indices = x_data_indices[:-1]\n",
    "        y_target_indices = x_data_indices[1:]\n",
    "        \n",
    "        x_source_vector[:len(x_source_indices)] = x_source_indices\n",
    "        y_target_vector[:len(y_target_indices)] = y_target_indices\n",
    "        \n",
    "        label_index = self.label_vocab[label]\n",
    "        \n",
    "        return x_source_vector, y_target_vector, label_index\n",
    "    \n",
    "    def transform(self, x_data_list, label_list):\n",
    "        \"\"\"Transform a dataset by vectorizing each datapoint\n",
    "        \n",
    "        Args: \n",
    "            x_data_list (list(list(str))): a list of lists, each sublist contains string tokens\n",
    "            label_list (list(str,int)): a list of either strings or integers. the label can come\n",
    "                as strings or integers, but they are remapped with the label_vocab to a unique integer\n",
    "        Returns:\n",
    "            numpy.ndarray, np.ndarray, np.ndarray: [\n",
    "                    shape=(dataset_size, max_seq_length), \n",
    "                    shape=(dataset_size, max_seq_length), \n",
    "                    shape=(dataset_size,)\n",
    "                ]\n",
    "                the first two outputs are x_data in two vectorized forms:\n",
    "                    the first is all of the source vectors (x_data[:-1])\n",
    "                    the second is all of the target vectors (x_data[1:])\n",
    "                the third output is the vector of labels mapped to the integers that represents them        \"\"\"\n",
    "        x_source_matrix = []\n",
    "        y_target_matrix = []\n",
    "        label_vector = []\n",
    "        \n",
    "        for x_data, label in zip(x_data_list, label_list):\n",
    "            x_source_vector, y_target_vector, label_index = self.vectorize(x_data, label)\n",
    "            x_source_matrix.append(x_source_vector)\n",
    "            y_target_matrix.append(y_target_vector)\n",
    "            label_vector.append(label_index)\n",
    "            \n",
    "        return np.stack(x_source_matrix), np.stack(y_target_matrix), np.stack(label_vector)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls, df, target_x_column, label_column, token_count_cutoff=0):\n",
    "        \"\"\"Instantiate the TextSequenceVectorizer from a standardized dataframe\n",
    "        \n",
    "        Standardized DataFrame has a special meaning:\n",
    "            there is a column that has been tokenized into a list of strings\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataset with a tokenized text column and a label column\n",
    "            target_x_column (str): the name of the tokenized text column\n",
    "            label_column (str): the name of the label column\n",
    "            token_count_cutoff (int): [default=0] the minimum token frequency to add to the\n",
    "                token_vocab.  Any tokens that are less frequent will not be added.\n",
    "        Returns:\n",
    "            TextSequenceVectorizer: the instantiated vectorizer\n",
    "        \"\"\"\n",
    "        # get the x data (the observations)\n",
    "        target_x_list = df[target_x_column].tolist()\n",
    "        # compute max sequence length, add 1 for the start/end token\n",
    "        max_seq_length = max(map(len, target_x_list)) + 1\n",
    "        \n",
    "        # populate token vocab        \n",
    "        token_vocab = Vocabulary(use_unks=False,\n",
    "                                 use_mask=True,\n",
    "                                 use_start_end=True,\n",
    "                                 start_token=START_TOKEN,\n",
    "                                 end_token=END_TOKEN)\n",
    "        counts = count_tokens(target_x_list)\n",
    "        # sort counts in reverse order\n",
    "        for token, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count <= token_count_cutoff:\n",
    "                break\n",
    "            token_vocab.add(token)\n",
    "        token_vocab.freeze()    \n",
    "        \n",
    "        \n",
    "        \n",
    "        # populate label vocab\n",
    "        label_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "        label_vocab.add_many(sorted(df[label_column].unique()))\n",
    "        label_vocab.freeze()\n",
    "        \n",
    "        return cls(token_vocab, label_vocab, max_seq_length)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the vectorizer using json to the file specified\n",
    "        \n",
    "        Args:\n",
    "            filename (str): the output file\n",
    "        \"\"\"\n",
    "        vec_dict = {\"token_vocab\": self.token_vocab.get_serializable_contents(),\n",
    "                    \"label_vocab\": self.label_vocab.get_serializable_contents(),\n",
    "                    \"max_seq_length\": self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        \"\"\"Load the vectorizer from the json file it was saved to\n",
    "        \n",
    "        Args:\n",
    "            filename (str): the file into which the vectorizer was saved.\n",
    "        Returns:\n",
    "            TextSequenceVectorizer: the instantiated vectorizer\n",
    "        \"\"\"\n",
    "        with open(filename, \"r\") as fp:\n",
    "            contents = json.load(fp)\n",
    "        \n",
    "        contents[\"token_vocab\"] = Vocabulary.deserialize_from_contents(contents[\"token_vocab\"])  \n",
    "        contents[\"token_vocab\"].freeze()\n",
    "        contents[\"label_vocab\"] = Vocabulary.deserialize_from_contents(contents[\"label_vocab\"])\n",
    "        \n",
    "        return cls(**contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Sequence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        vectorizer (SupervisedTextVectorizer): an instantiated vectorizer\n",
    "        active_split (str): the string name of the active split\n",
    "        \n",
    "        # internal use\n",
    "        _split_df (dict): a mapping from split name to partitioned DataFrame\n",
    "        _vectorized (dict): a mapping from split to an x data matrix and y vector\n",
    "        _active_df (pd.DataFrame): the DataFrame corresponding to the split\n",
    "        _active_source (np.ndarray): a matrix of the vectorized source text data\n",
    "        _active_target (np.ndarray): a matrix of the vectorized target text data\n",
    "        _active_labels (np.ndarray): a vector of the vectorized labels\n",
    "    \"\"\"\n",
    "    def __init__(self, df, vectorizer, target_x_column, label_column):\n",
    "        \"\"\"Initialize the TextSequenceDataset\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataset with a text and label column\n",
    "            vectorizer (TextSequenceVectorizer): an instantiated vectorizer\n",
    "            target_x_column (str): the column containing the tokenized text\n",
    "            label_column (str): the column containing the label\n",
    "        \"\"\"\n",
    "        self._split_df = {\n",
    "            'train': df[df.split=='train'],\n",
    "            'val': df[df.split=='val'],\n",
    "            'test': df[df.split=='test']\n",
    "        }\n",
    "        \n",
    "        self._vectorized = {}\n",
    "        for split_name, split_df in self._split_df.items():\n",
    "            self._vectorized[split_name] = \\\n",
    "                vectorizer.transform(x_data_list=split_df[target_x_column].tolist(), \n",
    "                                     label_list=split_df[label_column].str.lower().tolist())\n",
    "        self.vectorizer = vectorizer\n",
    "        self.active_split = None\n",
    "        self._active_df = None\n",
    "        self._active_source = None\n",
    "        self._active_target = None\n",
    "        self._active_labels = None\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    def set_split(self, split_name):\n",
    "        \"\"\"Set the active split\n",
    "        \n",
    "        Args:\n",
    "            split_name (str): the name of the split to make active; should\n",
    "                be one of 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self.active_split = split_name\n",
    "        self._active_source, self._active_target, self._active_labels = self._vectorized[split_name]\n",
    "        self._active_df = self._split_df[split_name]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return the data point corresponding to the index\n",
    "        \n",
    "        Args:\n",
    "            index (int): an int between 0 and len(self._active_source)\n",
    "        Returns:\n",
    "            dict: the data for this data point. Has the following form:\n",
    "                {\"x_source\": the vectorized source text data point, \n",
    "                 \"y_target\":  the vectorized target text data point,\n",
    "                 \"label_indices\": the index of the label for this data point,\n",
    "                 \"x_lengths\": method: the number of nonzeros in the vector,\n",
    "                 \"data_index\": the provided index for bookkeeping}\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"x_source\": self._active_source[index],\n",
    "            \"y_target\": self._active_target[index],\n",
    "            \"label_indices\": self._active_labels[index],\n",
    "            \"x_lengths\": len(self._active_source[index].nonzero()[0]),\n",
    "            \"data_index\": index\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"The length of the active dataset\n",
    "        \n",
    "        Returns:\n",
    "            int: len(self._active_x)\n",
    "        \"\"\"\n",
    "        return self._active_source.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_tokenizer(input_string):\n",
    "    \"\"\"Tokenized a string a list of its characters\n",
    "    \n",
    "    Args:\n",
    "        input_string (str): the character string to tokenize\n",
    "    Returns:\n",
    "        list: a list of characters\n",
    "    \"\"\"\n",
    "    return list(input_string)\n",
    "\n",
    "def load_surname_dataset(dataset_csv, tokenizer_func, saved_vectorizer_file=None):\n",
    "    \"\"\"Load the surname dataset \n",
    "    \n",
    "    Args:\n",
    "        dataset_csv (str): the location of the dataset\n",
    "        tokenizer_func (function): the tokenizing function to turn each datapoint into \n",
    "            its tokenized form\n",
    "        saved_vectorizer_file (str or None): [default=None] if not None, load the vectorizer\n",
    "            from the file\n",
    "    \"\"\"\n",
    "    df = add_splits(pd.read_csv(dataset_csv), \n",
    "                    target_y_column='nationality')\n",
    "    df['tokenized'] = df.surname.apply(tokenizer_func)\n",
    "    df['nationality'] = df.nationality.str.lower()\n",
    "    if saved_vectorizer_file is not None:\n",
    "        vectorizer = TextSequenceVectorizer.load(saved_vectorizer_file)\n",
    "    else:\n",
    "        vectorizer = TextSequenceVectorizer.from_df(df, \n",
    "                                                    target_x_column='tokenized', \n",
    "                                                    label_column='nationality',\n",
    "                                                    token_count_cutoff=0)\n",
    "    dataset = TextSequenceDataset(df=df, \n",
    "                                  vectorizer=vectorizer, \n",
    "                                  target_x_column='tokenized',\n",
    "                                  label_column='nationality')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_source': array([ 1, 31,  9,  4, 15,  8,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0]),\n",
       " 'y_target': array([  31,    9,    4,   15,    8,    6,    2, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]),\n",
       " 'label_indices': 0,\n",
       " 'x_lengths': 7,\n",
       " 'data_index': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_surname_dataset(\"../data/surnames.csv\", \n",
    "                               character_tokenizer)\n",
    "                               #\"../modelzoo/surnames.vectorizer\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_parameter(*size):\n",
    "    \"\"\"Initialize a new parameter\n",
    "    \n",
    "    Args:\n",
    "        size (*args): being star args, pass in any number of ints to create a \n",
    "            parameter tensor of that size.\n",
    "    Returns:\n",
    "        nn.Parameter: a Tensor that has some extra bookkeeping\n",
    "    \"\"\"\n",
    "    out = torch.randn(*size, requires_grad=True, dtype=torch.float32)\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return nn.Parameter(out)\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index.\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "def apply_across_sequence_looping(in_tensor, func, batch_first=True):\n",
    "    \"\"\"Apply a computational function over the sequence dimension in a tensor\n",
    "    \n",
    "    Args:\n",
    "        in_tensor (torch.FloatTensor): [\n",
    "            shape=(batch_size, max_seq_size, feature_size)\n",
    "            or shape=(max_seq_size, batch_size, feature_size)\n",
    "        ]\n",
    "        func (torch.nn.Module or function): either a PyTorch module (such as a Linear layer)\n",
    "            or a function; if a function, should output PyTorch tensors.  `func`\n",
    "            is to be applied to the feature vectors in `in_tensor` at each time step.\n",
    "    Returns:\n",
    "        torch.FloatTensor: the resultant from the `func` applied to each time step\n",
    "            The shape will be the same as the in_tensor except dim=2 will be whatever sized\n",
    "            vectors `func` outputs.\n",
    "    Notes:\n",
    "        This function applies `func` through the use of looping and is meant for explanation purposes.\n",
    "        For a faster runtime, use the reshape version. \n",
    "    \"\"\"\n",
    "    if batch_first:\n",
    "        in_tensor = in_tensor.permute(1, 0, 2)\n",
    "    \n",
    "    out_tensor = []\n",
    "    \n",
    "    for index in range(in_tensor.shape[0]):\n",
    "        out_i = func(in_tensor[index])\n",
    "        out_tensor.append(out_i)\n",
    "        \n",
    "    out_tensor = torch.stack(out_tensor)\n",
    "    \n",
    "    if batch_first:\n",
    "        out_tensor = out_tensor.permute(1, 0, 2)\n",
    "        \n",
    "    return out_tensor\n",
    "    \n",
    "def apply_across_sequence_reshape(in_tensor, func):\n",
    "    \"\"\"Apply a computational function over the sequence dimension in a tensor\n",
    "    \n",
    "    Args:\n",
    "        in_tensor (torch.FloatTensor): [\n",
    "            shape=(batch_size, max_seq_size, feature_size)\n",
    "            or shape=(max_seq_size, batch_size, feature_size)\n",
    "        ]\n",
    "        func (torch.nn.Module or function): either a PyTorch module (such as a Linear layer)\n",
    "            or a function; if a function, should output PyTorch tensors.  `func`\n",
    "            is to be applied to the feature vectors in `in_tensor` at each time step.\n",
    "    Returns:\n",
    "        torch.FloatTensor: the resultant from the `func` applied to each time step\n",
    "            The shape will be the same as the in_tensor except dim=2 will be whatever sized\n",
    "            vectors `func` outputs.\n",
    "    Notes:\n",
    "        This function applies `func` by reshaping `in_tensor` so that it appears to be a matrix,\n",
    "        then reshaping the resultant back to the expanded 3-dim shape.\n",
    "    \"\"\"\n",
    "    dim0, dim1, dim2 = in_tensor.size()\n",
    "    # reshape into a matrix so we can apply a linear layer\n",
    "    in_tensor = in_tensor.contiguous().view(dim0 * dim1, dim2)\n",
    "\n",
    "    # now that it's a matrix, can apply func\n",
    "    out_tensor = func(in_tensor) \n",
    "    \n",
    "    # return the tensor reshaped\n",
    "    return out_tensor.view(dim0, dim1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module: ExplicitRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitRNN(nn.Module):\n",
    "    \"\"\"An explicit implementation of the RNN for teaching purposes\n",
    "    \n",
    "    Attributes:\n",
    "        W_in2hid (nn.Parameter): the mapping from input features to hidden features\n",
    "        W_hid2hid (nn.Parameter): the mapping from hidden features to new hidden features\n",
    "        b_hid (nn.Parameter): the bias term for the hidden state computation\n",
    "        hidden_size (int): the size of the hidden state\n",
    "        batch_first (bool): [default=False] if True, the batch dimension\n",
    "            will be permuted to the dim=1 position and the sequence \n",
    "            dimension will take the dim=0 position for faster indexing\n",
    "            of the tensor. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"Initialize the ExplicitRNN\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): the size of the input feature vectors\n",
    "            hidden_size (int): the size of the RNN's hidden state\n",
    "            batch_first (bool): [default=False] if True, the batch dimension\n",
    "                will be permuted to the dim=1 position and the sequence \n",
    "                dimension will take the dim=0 position for faster indexing\n",
    "                of the tensor. \n",
    "        \"\"\"\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        \"\"\"The core update computation for the RNN\n",
    "        \n",
    "        Args:\n",
    "            x (torch.FloatTensor): [shape=(batch_size, input_size)] \n",
    "                The input data for the current time step (t)\n",
    "            h (torch.FloatTenosr): [shape=(batch_size, hidden_size)] \n",
    "                The hidden state for the previous time step (t-1)\n",
    "        Returns:\n",
    "            torch.FloatTensor: [shape=(batch_size, hidden_size)] \n",
    "                The hidden state for the the current time step (t)\n",
    "        \"\"\"\n",
    "        return torch.tanh(x.matmul(self.W_in2hid) + \n",
    "                          h.matmul(self.W_hid2hid) + \n",
    "                          self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        \"\"\"The forward computation of the RNN\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.FloatTensor): [shape=(batch_size,  seq_size, input_size) \n",
    "                or shape=(batch_size,  seq_size, input_size)]\n",
    "                The input data with batch either on dim=0 or dim=1. Conceptually,\n",
    "                this is a batch of sequences of vectors.  The default mode in PyTorch's\n",
    "                RNN, LSTM, and GRU implementations is to expect batch to be on dim=1\n",
    "                (as indicated by `batch_first`=False). This is usually done for speed or\n",
    "                simplicity in an algorithm. \n",
    "            hid_t (torch.FloatTensor): [default=None; shape=(batch_size, hidden_size)] \n",
    "                An optional hidden state; This can be used to condition or bias the RNN\n",
    "                to certain states. Some image captioning models use an image's feature vector\n",
    "                to bias an RNN to produce the captions. \n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        \n",
    "        if hid_t is None:\n",
    "            hid_t = torch.ones((batch_size, self.hidden_size))\n",
    "        if x_in.is_cuda:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            # assert x_t.shape == (batch_size, feat_size)\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            hiddens.append(hid_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConditionalCharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalCharRNN(nn.Module):\n",
    "    \"\"\"The Character-level RNN for sequence-predictions that conditions on initial hidden states\n",
    "    \n",
    "    Attributes:\n",
    "        emb (nn.Embedding): the embedding for tokens\n",
    "        conditional_emb (nn.Embedding): the embedding for the states to condition on\n",
    "        rnn (ExplicitRNN): the RNN implementation\n",
    "        fc (nn.Linear): a mapping from RNN outputs to the prediction vector\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, num_embeddings, num_conditioning_states, \n",
    "                 num_classes, hidden_size):\n",
    "        \"\"\"Initialize the CharRNN\n",
    "        \n",
    "        Args:\n",
    "            embedding_size (int): size of each embedding vector\n",
    "            num_embeddings (int): number of input characters\n",
    "            num_classes (int): number of characters to predict to\n",
    "            hidden_size (int): the intermediate representation size\n",
    "        \"\"\"\n",
    "        super(ConditionalCharRNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=num_embeddings,\n",
    "                                padding_idx=0)\n",
    "        \n",
    "        self.conditional_emb = nn.Embedding(embedding_dim=hidden_size, \n",
    "                                            num_embeddings=num_conditioning_states)\n",
    "        \n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, \n",
    "                               hidden_size=hidden_size, \n",
    "                               batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x_in, state_indices, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the sequence model\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): [shape=(batch_size, max_seq_length)]\n",
    "                The input data tensor. \n",
    "            state_indices (torch.Tensor): [shape=(batch_size,)]\n",
    "                The conditioning state indices for each batch item\n",
    "            apply_softmax (bool): [default=False] \n",
    "                A flag for the softmax activation.  This should be \n",
    "                false if used with the Cross Entropy losses. See note below.\n",
    "        Returns:\n",
    "            torch.FloatTensor: [shape=(batch_size, max_seq_length, num_classes)]\n",
    "                The vector for each data point in each sequence in the batch: \n",
    "                    if `apply_softmax=False`, it is the pre-softmax prediction vector\n",
    "                    else, it is the softmax'ed prediction vector\n",
    "        Note:\n",
    "            It is useful to not softmax the prediction vector because there is\n",
    "            a corresponding loss function optimized for it.  In essence, the loss\n",
    "            function associated with optimizing probabilities of multinomials is called\n",
    "            Negative Log Likelihood (NLL). To apply NLL, you first apply the log function.\n",
    "            This function cancels out with the exponential function of the softmax\n",
    "            and so some simplification can occur to shortcut the extra computations.\n",
    "        \"\"\"\n",
    "        # x_in.shape == (batch_size, max_seq_length)\n",
    "        \n",
    "        x_in = self.emb(x_in)\n",
    "        # x_in.shape == (batch_size, max_seq_length, embedding_size)\n",
    "\n",
    "        conditional_state_embedding = self.conditional_emb(state_indices)\n",
    "        \n",
    "        y_out = self.rnn(x_in, hid_t=conditional_state_embedding)\n",
    "        # y_out.shape == (batch_size, max_seq_length, hidden_size)\n",
    "\n",
    "        # apply the linear layer to each vector in each sequence\n",
    "        y_out = apply_across_sequence_looping(y_out, self.fc)\n",
    "        # alternatively:\n",
    "        # y_out = apply_across_sequence_reshape(y_out, self.fc)\n",
    "        # y_out.shape == (batch_size, max_seq_length, num_classes)\n",
    "        \n",
    "        # optionally apply the softmax to the last dim\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=2)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 21, 90])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(DataLoader(dataset, batch_size=8)))\n",
    "model = ConditionalCharRNN(embedding_size=8, \n",
    "               num_embeddings=len(dataset.vectorizer.token_vocab), \n",
    "               num_classes=len(dataset.vectorizer.token_vocab),\n",
    "               num_conditioning_states=len(dataset.vectorizer.label_vocab),\n",
    "               hidden_size=8)\n",
    "model(batch['x_source'], batch['label_indices']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index=IGNORE_INDEX_VALUE):\n",
    "    \"\"\"Compute the accuracy between a tensor of predictions and a matrix of label indices\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.FloatTensor): [shape=(batch_size, max_sequence_size, num_classes)]\n",
    "            The tensor of predictions, 1 prediction per batch item per sequence step\n",
    "        y_true (torch.FloatTensor): [shape=(batch_size, max_sequence_size)]\n",
    "            The matrix of label indices, 1 index per batch item per sequence step\n",
    "        mask_index (int): [default=IGNORE_INDEX_VALUE=-100]\n",
    "            the mask index is used to identify the positions in the y_true that \n",
    "            correspond to the masked positions. A negative number is suggested so that the\n",
    "            a label can have index 0. \n",
    "    Returns:\n",
    "        float: an accuracy computation that is sensitive to variable length sequences\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.contiguous().view(-1, y_pred.shape[2])\n",
    "    # y_pred.shape == (batch * sequence_length, prediction_vector)\n",
    "    y_true = y_true.contiguous().view(-1)\n",
    "    # y_true.shape == (batch * sequence_length, )\n",
    "\n",
    "    y_pred_indices = y_pred.argmax(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "\n",
    "def loss_func(y_pred, y_true, mask_index=IGNORE_INDEX_VALUE):\n",
    "    \"\"\"Compute the cross entropy loss sequence-wide with variable length handling\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.FloatTensor): [shape=(batch_size, max_sequence_size, num_classes)]\n",
    "            The tensor of predictions, 1 prediction per batch item per sequence step\n",
    "        y_true (torch.FloatTensor): [shape=(batch_size, max_sequence_size)]\n",
    "            The matrix of label indices, 1 index per batch item per sequence step\n",
    "        mask_index (int): [default=IGNORE_INDEX_VALUE=-100]\n",
    "            the mask index is used to identify the positions in the y_true that \n",
    "            correspond to the masked positions. A negative number is suggested so that the\n",
    "            a label can have index 0. \n",
    "    Returns:\n",
    "        torch.FloatTensor: a scalar representing the loss across the sequence\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.contiguous().view(-1, y_pred.shape[2])\n",
    "    # y_pred.shape == (batch * sequence_length, prediction_vector)\n",
    "    y_true = y_true.contiguous().view(-1)\n",
    "    # y_true.shape == (batch * sequence_length, )\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\", dataloader_kwargs=None): \n",
    "    \"\"\"Generate batches from a dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): the instantiated dataset\n",
    "        batch_size (int): the size of the batches\n",
    "        shuffle (bool): [default=True] batches are formed from shuffled indices\n",
    "        drop_last (bool): [default=True] don't return the final batch if it's smaller\n",
    "            than the specified batch size\n",
    "        device (str): [default=\"cpu\"] the device to move the tensors to\n",
    "        dataloader_kwargs (dict or None): [default=None] Any additional arguments to the\n",
    "            DataLoader can be specified\n",
    "    Yields:\n",
    "        dict: a dictionary mapping from tensor name to tensor object where the first\n",
    "            dimension of tensor object is the batch dimension\n",
    "    Note: \n",
    "        This function is mostly an iterator for the DataLoader, but has the added\n",
    "        feature that it moves the tensors to a target device. \n",
    "    \"\"\"\n",
    "    dataloader_kwargs = dataloader_kwargs or {}\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last, **dataloader_kwargs)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "\n",
    "class TrainState:\n",
    "    \"\"\"A data structure for managing training state operations.\n",
    "    \n",
    "    The TrainState will monitor validation loss and everytime a new best loss\n",
    "        (lower is better) is observed, a couple things happen:\n",
    "        \n",
    "        1. The model is checkpointed\n",
    "        2. Patience is reset\n",
    "    \n",
    "    Attributes:\n",
    "        model (torch.nn.Module): the model being trained and will be\n",
    "            checkpointed during training.\n",
    "        dataset (SupervisedTextDataset, TextSequenceDataset): the dataset \n",
    "            which is being iterate during training; must have the `active_split`\n",
    "            attribute. \n",
    "        log_dir (str): the directory to output the checkpointed model \n",
    "        patience (int): the number of epochs since a new best loss was observed\n",
    "        \n",
    "        # Internal Use\n",
    "        _full_model_path (str): `log_dir/model_state_file`\n",
    "        _split (str): the active split\n",
    "        _best_loss (float): the best observed loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataset, log_dir, model_state_file=\"model.pth\"):\n",
    "        \"\"\"Initialize the TrainState\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): the model to be checkpointed during training\n",
    "            dataset (SupervisedTextDataset, TextSequenceDataset): the dataset \n",
    "                which is being iterate during training; must have the `active_split`\n",
    "                attribute. \n",
    "            log_dir (str): the directory to output the checkpointed model \n",
    "            model_state_file (str): the name of the checkpoint model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self._full_model_path = os.path.join(log_dir, model_state_file)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self._metrics_by_split = {\n",
    "            'train': {}, \n",
    "            'val': {}, \n",
    "            'test': {}\n",
    "        }\n",
    "        \n",
    "        self._split = 'train'\n",
    "        self._best_loss = 10**10\n",
    "        self.patience = 0\n",
    "        \n",
    "    def _init_metric(self, split, metric_name):\n",
    "        \"\"\"Initialize a metric to the specified split\n",
    "        \n",
    "        A dictionary is created in `self._metrics_by_split` with\n",
    "            the keys 'running', 'count', and 'history'. \n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split to record the metric\n",
    "            metric_name (str): the name of the metric\n",
    "        \"\"\"\n",
    "        self._metrics_by_split[split][metric_name] = {\n",
    "            'running': 0.,\n",
    "            'count': 0,\n",
    "            'history': []\n",
    "        }\n",
    "        \n",
    "    def _update_metric(self, metric_name, metric_value):\n",
    "        \"\"\"Update a metric with an observed value\n",
    "        \n",
    "        Specifically, the running average is updated.\n",
    "        \n",
    "        Args:\n",
    "            metric_name (str): the name of the metric\n",
    "            metric_value (float): the observed value of the metric\n",
    "        \"\"\"\n",
    "        if metric_name not in self._metrics_by_split[self._split]:\n",
    "            self._init_metric(self._split, metric_name)\n",
    "        metric = self._metrics_by_split[self._split][metric_name]\n",
    "        metric['count'] += 1\n",
    "        metric['running'] += (metric_value - metric['running']) / metric['count']\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        \"\"\"Set the dataset split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split to set\n",
    "        \"\"\"\n",
    "        self._split = split\n",
    "        \n",
    "    def get_history(self, split, metric_name):\n",
    "        \"\"\"Get the history of values for any metric in any split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split\n",
    "            metric_name (str): the target metric\n",
    "            \n",
    "        Returns:\n",
    "            list(float): the running average of each epoch for `metric_name` in `split` \n",
    "        \"\"\"\n",
    "        return self._metrics_by_split[split][metric_name]['history']\n",
    "    \n",
    "    def get_value_of(self, split, metric_name):\n",
    "        \"\"\"Retrieve the running average of any metric in any split\n",
    "        \n",
    "        Args:\n",
    "            split (str): the target split\n",
    "            metric_name (str): the target metric\n",
    "            \n",
    "        Returns:\n",
    "            float: the running average for `metric_name` in `split`\n",
    "        \"\"\"\n",
    "        return self._metrics_by_split[split][metric_name]['running']\n",
    "        \n",
    "    def log_metrics(self, **metrics):\n",
    "        \"\"\"Log some values for some metrics\n",
    "        \n",
    "        Args:\n",
    "            metrics (kwargs): pass keyword args with the form `metric_name=metric_value`\n",
    "                to log the metric values into the attribute `_metrics_by_split`.\n",
    "        \"\"\"\n",
    "        self._split = self.dataset.active_split\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            self._update_metric(metric_name, metric_value)\n",
    "            \n",
    "    def log_epoch_end(self):\n",
    "        \"\"\"Log the end of the epoch. \n",
    "        \n",
    "        Some key functions happen at the end of the epoch:\n",
    "            - for each metric in each split running averages, counts, \n",
    "              and history are updated\n",
    "            - the model is checkpointed if a new best value is observed\n",
    "            - patience is incremented if a new best value is not observed\n",
    "        \"\"\"\n",
    "        for split_dict in self._metrics_by_split.values():\n",
    "            for metric_dict in split_dict.values():\n",
    "                metric_dict['history'].append(metric_dict['running'])\n",
    "                metric_dict['running'] = 0.0\n",
    "                metric_dict['count'] = 0\n",
    "                \n",
    "        if 'loss' in self._metrics_by_split['val']:\n",
    "            val_loss = self._metrics_by_split['val']['loss']['history'][-1]\n",
    "            if val_loss < self._best_loss:\n",
    "                self._best_loss = val_loss\n",
    "                self.save_model()\n",
    "                self.patience = 0\n",
    "            else:\n",
    "                self.patience += 1\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" Save `model` to `log_dir/model_state_file` \"\"\"\n",
    "        torch.save(self.model.state_dict(), self._full_model_path)\n",
    "    \n",
    "    def reload_best(self):\n",
    "        \"\"\" reload `log_dir/model_state_file` to `model` \"\"\"\n",
    "        if os.path.exists(self._full_model_path):\n",
    "            self.model.load_state_dict(torch.load(self._full_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Dataset\n",
    "    surname_csv=\"../data/surnames.csv\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=16,\n",
    "    hidden_size=64,\n",
    "    num_classes=-1,\n",
    "    num_conditioning_states=-1,\n",
    "    num_embeddings=-1,\n",
    "    # Training options\n",
    "    batch_size = 128,\n",
    "    cuda=False,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    patience_threshold=3,\n",
    "    # model reload options\n",
    "    load_zoo_model=True,\n",
    "    zoo={\n",
    "        'model': '../modelzoo/charnn_emb16_hid64_surnames_conditionally_predict.pth',\n",
    "        'vectorizer': '../modelzoo/surnames.vectorizer',\n",
    "        'comments': 'pre-trained surname conditioned sequence prediction (& conditioned generation)',\n",
    "        'parameters': {\n",
    "            'embedding_size': 16,\n",
    "            'hidden_size': 64\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading zoo model: True\n"
     ]
    }
   ],
   "source": [
    "args.load_zoo_model = True\n",
    "\n",
    "args.load_zoo_model = (\n",
    "    args.load_zoo_model \n",
    "    and os.path.exists(args.zoo['vectorizer']) \n",
    "    and os.path.exists(args.zoo['model'])\n",
    ")\n",
    "\n",
    "print(f\"Loading zoo model: {args.load_zoo_model}\")\n",
    "\n",
    "if args.load_zoo_model:\n",
    "    dataset = load_surname_dataset(dataset_csv=args.surname_csv, \n",
    "                                   tokenizer_func=character_tokenizer, \n",
    "                                   saved_vectorizer_file=args.zoo['vectorizer'])\n",
    "    args.embedding_size = args.zoo['parameters']['embedding_size']\n",
    "    args.hidden_size = args.zoo['parameters']['hidden_size']\n",
    "else:\n",
    "    dataset = load_surname_dataset(dataset_csv=args.surname_csv, \n",
    "                                   tokenizer_func=character_tokenizer)\n",
    "\n",
    "args.num_embeddings = len(dataset.vectorizer.token_vocab)\n",
    "args.num_classes = len(dataset.vectorizer.token_vocab)\n",
    "args.num_conditioning_states = len(dataset.vectorizer.label_vocab)\n",
    "\n",
    "model = ConditionalCharRNN(embedding_size=args.embedding_size, \n",
    "               hidden_size=args.hidden_size,\n",
    "               num_embeddings=args.num_embeddings,\n",
    "               num_classes=args.num_classes,\n",
    "               num_conditioning_states=args.num_conditioning_states)\n",
    "\n",
    "if args.load_zoo_model:\n",
    "    model.load_state_dict(\n",
    "        torch.load(args.zoo['model'], map_location=lambda storage, loc: storage)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdddb0a0d644f22834d1dfb6f795b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', style=ProgressStyle(description_width='initial')), H"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2eb99b0a974bc2badcf15956eea2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=109, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1a7e95f5d24fb5bd7096d20138968d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validation', max=23, style=ProgressStyle(description_width='i"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847e409715b04723851be3f3f436358f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test', max=23, style=ProgressStyle(description_width='initial"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.to(args.device)\n",
    "\n",
    "train_state = TrainState(model, dataset=dataset, log_dir='./logs/conditional_charrnn_predict_surnames', \n",
    "                         model_state_file='model.pth')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# loss function with class-weighted modifications\n",
    "# loss_func is defined in the training utilities. \n",
    "# It is a sequence-wide cross entropy loss with special handling for masked predictions\n",
    "\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_bar = tqdm_notebook(desc='training', total=len(dataset)//args.batch_size)\n",
    "\n",
    "dataset.set_split(\"val\")\n",
    "val_bar = tqdm_notebook(desc='validation', total=len(dataset)//args.batch_size)\n",
    "        \n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        model.train()\n",
    "        dataset.set_split(\"train\")\n",
    "        # TODO: deprecate in favor of single source of truth\n",
    "        train_state.set_split(\"train\")\n",
    "        \n",
    "        for batch in generate_batches(dataset, batch_size=args.batch_size, device=args.device):\n",
    "            # Step 1: clear the gradients \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Step 2: compute the outputs\n",
    "            y_prediction = model(batch['x_source'], batch['label_indices'])\n",
    "\n",
    "            # Step 3: compute the loss\n",
    "            loss = loss_func(y_prediction, batch['y_target'])\n",
    "            \n",
    "            # Step 4: propagate the gradients\n",
    "            loss.backward() \n",
    "            \n",
    "            # Step 5: update the model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Auxillary: logging\n",
    "            train_state.log_metrics(loss=loss.item(), \n",
    "                                    accuracy=compute_accuracy(y_prediction, batch['y_target']))\n",
    "            \n",
    "            train_bar.set_postfix(loss=train_state.get_value_of(split=\"train\", metric_name=\"loss\"),\n",
    "                                  acc=train_state.get_value_of(split=\"train\", metric_name=\"accuracy\"))\n",
    "            train_bar.update()\n",
    "            \n",
    "        # loop over test dataset\n",
    "        \n",
    "        model.eval()\n",
    "        dataset.set_split(\"val\")\n",
    "        train_state.set_split(\"val\")\n",
    "        \n",
    "        for batch in generate_batches(dataset, batch_size=args.batch_size, device=args.device):\n",
    "            # Step 1: compute the outputs\n",
    "            y_prediction = model(batch['x_source'], batch['label_indices'])\n",
    "\n",
    "            # Step 2: compute the loss\n",
    "            loss = loss_func(y_prediction, batch['y_target'])\n",
    "            \n",
    "            # Auxillary: logging\n",
    "            train_state.log_metrics(loss=loss.item(), \n",
    "                                    accuracy=compute_accuracy(y_prediction, batch['y_target']))\n",
    "            \n",
    "            val_bar.set_postfix(loss=train_state.get_value_of(split=\"val\", metric_name=\"loss\"),\n",
    "                                  acc=train_state.get_value_of(split=\"val\", metric_name=\"accuracy\"))\n",
    "            val_bar.update()\n",
    "\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_state.get_value_of(split=\"train\", \n",
    "                                                                  metric_name=\"loss\"), \n",
    "                              train_accuracy=train_state.get_value_of(split=\"train\", \n",
    "                                                                      metric_name=\"accuracy\"),\n",
    "                              val_loss=train_state.get_value_of(split=\"val\", \n",
    "                                                                metric_name=\"loss\"), \n",
    "                              val_accuracy=train_state.get_value_of(split=\"val\", \n",
    "                                                                    metric_name=\"accuracy\"),\n",
    "                              patience=train_state.patience)\n",
    "        epoch_bar.update()\n",
    "        train_state.log_epoch_end()\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        \n",
    "        if train_state.patience > args.patience_threshold:\n",
    "            break\n",
    "            \n",
    "    train_state.reload_best()\n",
    "    model.eval()\n",
    "    dataset.set_split(\"test\")\n",
    "    test_bar = tqdm_notebook(desc='test', total=len(dataset)//args.batch_size)\n",
    "\n",
    "    for batch in generate_batches(dataset, batch_size=args.batch_size, device=args.device):\n",
    "        # Step 1: compute the outputs\n",
    "        y_prediction = model(batch['x_source'], batch['label_indices'])\n",
    "\n",
    "        # Step 2: compute the loss\n",
    "        loss = loss_func(y_prediction, batch['y_target'])\n",
    "\n",
    "        # Auxillary: logging\n",
    "        train_state.log_metrics(loss=loss.item(), \n",
    "                                accuracy=compute_accuracy(y_prediction, batch['y_target']))\n",
    "\n",
    "        test_bar.set_postfix(loss=train_state.get_value_of(split=\"test\", metric_name=\"loss\"),\n",
    "                             acc=train_state.get_value_of(split=\"test\", metric_name=\"accuracy\"))\n",
    "        test_bar.update()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model to Sample\n",
    "\n",
    "We can define a new model which doesn't change the stored modules/parameters and define a new forward function.\n",
    "\n",
    "Then, we can use that forward function to generate outputs sampled from the model's predictions.\n",
    "\n",
    "One method, `interpolated_sample` has been left only partially complete for you to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingCharRNN(ConditionalCharRNN):\n",
    "    def sample(self, initial_string, nationality, token_vocab, label_vocab, max_length=30, temperature=1):\n",
    "        \"\"\"Sample from the model in a human-interfacing way\n",
    "        \n",
    "        Args:\n",
    "            initial_string (str): the initial letters of the surname to be generated. Passing an empty\n",
    "                string is acceptable.\n",
    "            nationality (str): one of the nationalities to condition on; must be in `label_vocab`\n",
    "            token_vocab (Vocabulary): the vocabulary that maps tokens to integers\n",
    "            max_length (int): the maximum length to generate\n",
    "            temperature (float): module the peakiness of the probability distributions using\n",
    "                temperature; see the Boltzmann Distribution (https://en.wikipedia.org/wiki/Boltzmann_distribution)\n",
    "        Returns:\n",
    "            str: the generated sequence of characters (including the seeded `initial_string`)\n",
    "        \"\"\"\n",
    "        if len(initial_string) == 0 or initial_string[0] != token_vocab.start_token:\n",
    "            initial_string = token_vocab.start_token + initial_string\n",
    "        \n",
    "        x_data = torch.LongTensor([token_vocab[token] for token in initial_string]).view(1, -1)\n",
    "        \n",
    "        initial_hidden_indices = torch.LongTensor([label_vocab[nationality]]).view(-1)\n",
    "        hid_t = self.conditional_emb(initial_hidden_indices)\n",
    "        \n",
    "        output_indices = self.forward(x_data, hid_t, max_length, temperature).view(-1).detach().numpy()\n",
    "        \n",
    "        # remove start token\n",
    "        output_indices = output_indices[1:]\n",
    "        output = \"\"\n",
    "        for index in output_indices:\n",
    "            if index == token_vocab.end_index:\n",
    "                break\n",
    "            output += token_vocab.lookup(index)\n",
    "        return output\n",
    "    \n",
    "    def interpolated_sample(self, initial_string, nationality1, nationality2, interpolation_weight, \n",
    "                            token_vocab, label_vocab, max_length=30, temperature=1):\n",
    "        \"\"\"Sample from a state conditioned on 2 interpolated nationalities\n",
    "        \n",
    "        Args:\n",
    "            initial_string (str): the initial letters of the surname to be generated. Passing an empty\n",
    "                string is acceptable.\n",
    "            nationality1 (str): one of the nationalities to condition on; must be in `label_vocab`\n",
    "            nationality2 (str): one of the nationalities to condition on; must be in `label_vocab`\n",
    "            interpolation_weight (float): [0 < interpolation_weight < 1]\n",
    "                The amount to interpolate between the two nationalities; \n",
    "                `interpolation_weight * nationality1 + (1 - interpolation_weight) * nationality2`\n",
    "            token_vocab (Vocabulary): the vocabulary that maps tokens to integers\n",
    "            max_length (int): the maximum length to generate\n",
    "            temperature (float): module the peakiness of the probability distributions using\n",
    "                temperature; see the Boltzmann Distribution (https://en.wikipedia.org/wiki/Boltzmann_distribution)\n",
    "        Returns:\n",
    "            str: the generated sequence of characters (including the seeded `initial_string`)\n",
    "        \"\"\"\n",
    "        if len(initial_string) == 0 or initial_string[0] != token_vocab.start_token:\n",
    "            initial_string = token_vocab.start_token + initial_string\n",
    "        \n",
    "        x_data = torch.LongTensor([token_vocab[token] for token in initial_string]).view(1, -1)\n",
    "        \n",
    "        ###\n",
    "        # COMPUTE THE INTERPOLATED NATIONALITY HERE\n",
    "        ###\n",
    "        \n",
    "        output_indices = self.forward(x_data, hid_t, max_length, temperature).view(-1).detach().numpy()\n",
    "        \n",
    "        # remove start token\n",
    "        output_indices = output_indices[1:]\n",
    "        output = \"\"\n",
    "        for index in output_indices:\n",
    "            if index == token_vocab.end_index:\n",
    "                break\n",
    "            output += token_vocab.lookup(index)\n",
    "        return output\n",
    "            \n",
    "    def forward(self, x_source, hid_t, max_length, temperature=1):\n",
    "        \"\"\"Compute a new forward pass which samples from the representations\n",
    "        \n",
    "        Args:\n",
    "            x_source (torch.LongTensor): The indices to seed the sampling\n",
    "            initial_hidden_indices (torch.LongTensor): The indices for the conditioning state\n",
    "            max_length (int): the number of samples to compute\n",
    "            temperature (float): module the peakiness of the probability distributions using\n",
    "                temperature; see the Boltzmann Distribution (https://en.wikipedia.org/wiki/Boltzmann_distribution)\n",
    "        \"\"\"\n",
    "        \n",
    "        # get to the hidden state needed to generate by applying the rnn to the input\n",
    "        # note: this could have also been done by applying the rnn to x_source \n",
    "        x_source = x_source.permute(1, 0)\n",
    "        for t in range(x_source.shape[0]):\n",
    "            indices_t = x_source[t]\n",
    "            x_embedding_t =  self.emb(indices_t)\n",
    "            hid_t = self.rnn._compute_next_hidden(x_embedding_t, hid_t)\n",
    "        \n",
    "        # compute the initial indices\n",
    "        prediction_t = F.softmax(self.fc(hid_t), dim=1)\n",
    "        \n",
    "        indices_t = torch.multinomial(prediction_t, num_samples=1).view(-1)\n",
    "        \n",
    "        # start caching the indices\n",
    "        generated_indices = [indices_t]\n",
    "        \n",
    "        for t in range(max_length - x_source.shape[0]):\n",
    "            # embed the indices\n",
    "            x_embedded_t = self.emb(indices_t)\n",
    "            # compute the next hidden\n",
    "            hid_t = self.rnn._compute_next_hidden(x_embedded_t, hid_t)\n",
    "            # compute the probability distribution by passing the hidden through the linear layer\n",
    "            # and applying the softmax function\n",
    "            prediction_t = F.softmax(self.fc(hid_t) / temperature, dim=1)\n",
    "            # use torch.multinomial to sample from the probability distribution\n",
    "            indices_t = torch.multinomial(prediction_t, num_samples=1).view(-1)\n",
    "            # cache the resulting indices\n",
    "            generated_indices.append(indices_t)\n",
    "            \n",
    "        # concatenate the passed in tensor with the generated one\n",
    "        return torch.cat([x_source.permute(1, 0), torch.stack(generated_indices, dim=1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SamplingCharRNN(embedding_size=args.embedding_size, \n",
    "                        hidden_size=args.hidden_size,\n",
    "                        num_embeddings=args.num_embeddings,\n",
    "                        num_classes=args.num_classes,\n",
    "                        num_conditioning_states=args.num_conditioning_states)\n",
    "train_state.model = model\n",
    "train_state.reload_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: arabic, russian, czech, japanese, english, german, portuguese, italian, greek, french, dutch, irish, chinese, spanish, korean, scottish, vietnamese, polish\n"
     ]
    }
   ],
   "source": [
    "print(f\"Options: {', '.join(dataset.vectorizer.label_vocab.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Borferen'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(initial_string=\"\", \n",
    "             nationality=\"german\", \n",
    "             token_vocab=dataset.vectorizer.token_vocab, \n",
    "             label_vocab=dataset.vectorizer.label_vocab,\n",
    "             max_length=20, \n",
    "             temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic -> Hana\n",
      "chinese -> Chan\n",
      "czech -> Hovimo\n",
      "dutch -> Asteis\n",
      "english -> Hergharag\n",
      "french -> Lessigv\n",
      "german -> Kachley\n",
      "greek -> Kontari\n",
      "irish -> O'Plad\n",
      "italian -> Racpur\n",
      "japanese -> Towada\n",
      "korean -> Shek\n",
      "polish -> Hodze\n",
      "portuguese -> Carto\n",
      "russian -> Mihevets\n",
      "scottish -> Witt\n",
      "spanish -> Vortili\n",
      "vietnamese -> Hen\n"
     ]
    }
   ],
   "source": [
    "for nationality in sorted(dataset.vectorizer.label_vocab.keys()):\n",
    "    generated_surname = model.sample(initial_string=\"\", \n",
    "                                     nationality=nationality, \n",
    "                                     token_vocab=dataset.vectorizer.token_vocab, \n",
    "                                     label_vocab=dataset.vectorizer.label_vocab,\n",
    "                                     max_length=20, \n",
    "                                     temperature=0.9)\n",
    "    print(f\"{nationality} -> {generated_surname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magis",
   "language": "python",
   "name": "magis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
